{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import h5py \n",
    "import numpy as np\n",
    "\n",
    "PATH_DATA = 'data/eighth.h5'\n",
    "PATH_PREDICT_WITHOUT_GT = 'data/pred_students/pred_eighties_from_half_1_without_gt.h5'\n",
    "PATH_SUBMIT = 'data/submit/pred_eighties_from_half_1_AWESOMEGROUP.h5'\n",
    "PATH_PREDICT_WITH_GT = 'data/pred_teachers/pred_eighties_from_half_1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "class DataGenerator(object):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    def __init__(self, data, database_size, img_height, img_width, num_classes, num_input_channels, batch_size = 32, perc_test = 0.3, shuffle = True):\n",
    "        'Initialization'\n",
    "        self.data = data\n",
    "        self.database_size = database_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.perc_test = perc_test\n",
    "        self.perc_train = 1.0 - perc_test\n",
    "\n",
    "        'Dataset split'\n",
    "        list_IDs = np.arange(self.database_size)\n",
    "        self.train_IDs = list_IDs[:int(self.perc_train * self.database_size)]\n",
    "        self.test_IDs = list_IDs[int(self.perc_train * self.database_size):]\n",
    "\n",
    "    def frequency_balancing(self):\n",
    "        'Median frequency balacing'\n",
    "        # Initialize class weights\n",
    "        class_weights = dict()\n",
    "\n",
    "        # Initialize class frequencies\n",
    "        class_frequencies = np.zeros(num_classes,)\n",
    "\n",
    "        # For every picture inthe dataset, compute every class frequency\n",
    "        for i in range(self.database_size):\n",
    "            y = to_categorical(np.reshape(self.data['TOP_LANDCOVER'][i], (-1, 1)), num_classes=self.num_classes)\n",
    "            class_frequencies[np.argmax(y)] += 1\n",
    "\n",
    "        # Compute classes weights\n",
    "        class_frequencies /= database_size\n",
    "\n",
    "        for j in range(self.num_classes):\n",
    "            if class_frequencies[j] != 0:\n",
    "                class_weights[j] = np.median(class_frequencies) / class_frequencies[j]\n",
    "            else:\n",
    "                class_weights[j] = 0\n",
    "\n",
    "        return class_weights\n",
    "\n",
    "    def generate_train(self):\n",
    "        'Generates batches of samples'\n",
    "        # Infinite loop\n",
    "        while 1:\n",
    "            # Generate order of exploration of dataset\n",
    "            list_IDs = self.train_IDs\n",
    "\n",
    "            # Generate batches\n",
    "            imax = int(len(list_IDs)/self.batch_size)\n",
    "            idxs = np.arange(imax)\n",
    "            \n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "                \n",
    "            for i in idxs:\n",
    "                # Find list of IDs                \n",
    "                list_IDs_temp = list_IDs[i*self.batch_size:(i+1)*self.batch_size]\n",
    "\n",
    "                # Generate data\n",
    "                X, Y = self.__data_generation(list_IDs_temp)\n",
    "                yield X, Y\n",
    "\n",
    "    def generate_test(self):\n",
    "        'Generates batches of samples'\n",
    "        # Infinite loop\n",
    "        while 1:\n",
    "            # Generate order of exploration of dataset\n",
    "            list_IDs = self.test_IDs\n",
    "\n",
    "            # Generate batches\n",
    "            imax = int(len(list_IDs)/self.batch_size)\n",
    "            idxs = np.arange(imax)\n",
    "            \n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "                \n",
    "            for i in idxs:\n",
    "                # Find list of IDs\n",
    "                list_IDs_temp = list_IDs[i*self.batch_size:(i+1)*self.batch_size]\n",
    "\n",
    "                # Generate data\n",
    "                X, Y = self.__data_generation(list_IDs_temp)\n",
    "                yield X, Y\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data of batch_size samples' # X : (n_samples, v_size, v_size, v_size, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, self.img_height, self.img_width, self.num_input_channels))\n",
    "        Y = np.zeros((self.batch_size, NUM_CLASSES))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store image\n",
    "            X[i, :, :, :] = self.data['S2'][ID]\n",
    "            # Store label vector\n",
    "            Y[i, :] = to_categorical(self.data['TOP_LANDCOVER'][ID], num_classes=self.num_classes)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D, Conv2D, BatchNormalization, UpSampling2D, Dense\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pictures dimensions\n",
    "NB_CHANNELS_INPUTS = 4\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 23\n",
    "\n",
    "IMG_HEIGHT = 16\n",
    "IMG_WIDTH = 16\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, NB_CHANNELS_INPUTS))\n",
    "conv1 = BatchNormalization()(inputs)\n",
    "\n",
    "#First,let us build the encoder network\n",
    "conv1 = Conv2D(filters=16, kernel_size=(2, 2), padding=\"same\")(conv1)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "conv1 = Activation('relu')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(conv1)\n",
    "\n",
    "conv2 = Conv2D(filters=32, kernel_size=(2, 2), padding=\"same\")(pool1)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "conv2 = Activation('relu')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(conv2)\n",
    "\n",
    "flat = Flatten()(pool2)\n",
    "predictions = Dense(NUM_CLASSES, activation = \"softmax\")(flat)\n",
    "\n",
    "#Finally, let us build the model\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_data = h5py.File(PATH_DATA, \"r\")\n",
    "database_size = training_data['S2'].shape[0]\n",
    "img_height = training_data['S2'].shape[1]\n",
    "img_width = training_data['S2'].shape[2]\n",
    "batch_size = 32\n",
    "nb_epochs = 5\n",
    "num_classes = NUM_CLASSES\n",
    "num_input_channels = NB_CHANNELS_INPUTS\n",
    "perc_test = 0.3\n",
    "perc_train = 1.0 - perc_test\n",
    "\n",
    "# Instanciate generator\n",
    "data_gen = DataGenerator(data = training_data, database_size = database_size, img_height = img_height, img_width = img_width, num_classes = num_classes, num_input_channels = num_input_channels, batch_size = batch_size, perc_test = perc_test)\n",
    "\n",
    "# Get class weights balancy based on the training db\n",
    "class_weight = data_gen.frequency_balancing()\n",
    "print(\"class_weight : \", class_weight)\n",
    "\n",
    "np.save('eighth_weigths.npz', class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = np.load('eighth_weigths.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 0, 1: 5.452017700884136e-05, 2: 2.2951308798384227e-05, 3: 1.5047512520784376e-05, 4: 3.780170485688905e-05, 5: 9.29195316855603e-06, 6: 1.0, 7: 0, 8: 0, 9: 0, 10: 0.0008384572386808273, 11: 0.0084985835694051, 12: 1.1775534756472127e-05, 13: 0, 14: 0.008368200836820085, 15: 0, 16: 0, 17: 3.0, 18: 0, 19: 0.014527845036319612, 20: 2.4580593621335954e-05, 21: 0, 22: 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 16, 16, 4)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 4)         16        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 16)        272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 32)          2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 23)                11799     \n",
      "=================================================================\n",
      "Total params: 14,359\n",
      "Trainable params: 14,255\n",
      "Non-trainable params: 104\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "    5/14608 [..............................] - ETA: 2:03:41 - loss: 7.3935e-05 - categorical_accuracy: 0.0109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.101864). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  537/14608 [>.............................] - ETA: 39:05 - loss: 7.6492e-04 - categorical_accuracy: 0.1261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.104407). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.114931). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6260/14608 [===========>..................] - ETA: 22:38 - loss: 1.3059e-04 - categorical_accuracy: 0.2185- ETA: 22:41 - loss: 1.3093e-04 - categor"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.101849). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14608/14608 [==============================] - 2422s 166ms/step - loss: 1.2574e-04 - categorical_accuracy: 0.2737 - val_loss: 3.2371 - val_categorical_accuracy: 0.0135\n",
      "Epoch 2/5\n",
      "14608/14608 [==============================] - 2189s 150ms/step - loss: 9.4426e-05 - categorical_accuracy: 0.3464 - val_loss: 2.4535 - val_categorical_accuracy: 0.0437\n",
      "Epoch 3/5\n",
      " 1659/14608 [==>...........................] - ETA: 29:49 - loss: 1.7020e-04 - categorical_accuracy: 0.3380"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.101644). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5774/14608 [==========>...................] - ETA: 20:28 - loss: 8.1568e-05 - categorical_accuracy: 0.3677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.110896). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14608/14608 [==============================] - 2164s 148ms/step - loss: 9.8546e-05 - categorical_accuracy: 0.3745 - val_loss: 2.1604 - val_categorical_accuracy: 0.1570\n",
      "Epoch 4/5\n",
      "    3/14608 [..............................] - ETA: 18:36 - loss: 3.9265e-05 - categorical_accuracy: 0.2266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.110641). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4588/14608 [========>.....................] - ETA: 23:19 - loss: 1.0626e-04 - categorical_accuracy: 0.3639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.105377). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12082/14608 [=======================>......] - ETA: 5:53 - loss: 8.4502e-05 - categorical_accuracy: 0.3797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.104862). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14608/14608 [==============================] - 2162s 148ms/step - loss: 7.7781e-05 - categorical_accuracy: 0.3778 - val_loss: 2.5296 - val_categorical_accuracy: 0.0436: 7.7807e-05 - categorical_ac\n",
      "Epoch 5/5\n",
      "    3/14608 [..............................] - ETA: 23:55 - loss: 1.8949e-05 - categorical_accuracy: 0.4505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.114303). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10766/14608 [=====================>........] - ETA: 8:57 - loss: 6.0056e-05 - categorical_accuracy: 0.3870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.100154). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14608/14608 [==============================] - 2148s 147ms/step - loss: 7.4308e-05 - categorical_accuracy: 0.3825 - val_loss: 2.0836 - val_categorical_accuracy: 0.1337\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "training_data = h5py.File(PATH_DATA, \"r\")\n",
    "database_size = training_data['S2'].shape[0]\n",
    "img_height = training_data['S2'].shape[1]\n",
    "img_width = training_data['S2'].shape[2]\n",
    "batch_size = 128\n",
    "nb_epochs = 5\n",
    "num_classes = NUM_CLASSES\n",
    "num_input_channels = NB_CHANNELS_INPUTS\n",
    "perc_test = 0.2\n",
    "perc_train = 1.0 - perc_test\n",
    "\n",
    "# Instanciate generator\n",
    "data_gen = DataGenerator(data = training_data, database_size = database_size, img_height = img_height, img_width = img_width, num_classes = num_classes, num_input_channels = num_input_channels, batch_size = batch_size, perc_test = perc_test)\n",
    "\n",
    "# Create the training and testing generators\n",
    "train_generator = data_gen.generate_train()\n",
    "validation_generator = data_gen.generate_test()\n",
    "\n",
    "#Visualize model\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=losses.categorical_crossentropy, optimizer='adam', metrics = ['categorical_accuracy'])\n",
    "\n",
    "# Train model on dataset\n",
    "callback = model.fit_generator(generator=train_generator, class_weight = class_weight, steps_per_epoch=int(perc_train*database_size)//batch_size, epochs=nb_epochs, validation_data=validation_generator, validation_steps=int(0.2*perc_test*database_size//batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('second_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
